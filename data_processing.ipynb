{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import re\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/avatar.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Dictionary to store character connections & chapters \n",
    "character_connections = {}\n",
    "chapter_connections = {}\n",
    "# Dictionary to store character connections by book\n",
    "character_connections_by_book = {}\n",
    "\n",
    "\n",
    "scene_characters = set()  # Characters in the current scene\n",
    "\n",
    "\n",
    "# Name replacements for normalization\n",
    "name_replacements = {\n",
    "    'young zuko': 'zuko',\n",
    "    'young azula': 'azula',\n",
    "    'young katara': 'katara',\n",
    "    'young sokka': 'sokka',\n",
    "    'young toph': 'toph',\n",
    "    'young aang': 'aang',\n",
    "    'king bumi': 'bumi',\n",
    "    'avatar roku': 'roku',\n",
    "    'avatar kyoshi': 'kyoshi',\n",
    "    'avatar kuruk': 'kuruk',\n",
    "    'avatar yangchen': 'yangchen',\n",
    "    'aang:': 'aang',\n",
    "    'sha-mo:': 'sha-mo',    \n",
    "}\n",
    "\n",
    "# Exclude these words from being counted as characters\n",
    "invalid_characters = {'together', 'both'}\n",
    "\n",
    "# Function to normalize character names\n",
    "def normalize_name(name):\n",
    "    # Convert to lowercase, strip spaces, and standardize\n",
    "    normalized = name.lower().strip()\n",
    "    return name_replacements.get(normalized, normalized)\n",
    "\n",
    "# Function to split multiple characters and normalize their names\n",
    "def split_characters(character):\n",
    "    # Replace \"Team Avatar\" with its members\n",
    "    if 'team avatar' in character.lower():\n",
    "        return ['sokka', 'katara', 'aang', 'toph']\n",
    "    \n",
    "    # Use regex to split by commas or 'and', and normalize each name\n",
    "    names = [normalize_name(name) for name in re.split(r',|\\band\\b', character)]\n",
    "    # Filter out invalid characters\n",
    "    return [name for name in names if name not in invalid_characters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene Boundary Detection and Character Network Analysis\n",
    "\n",
    "This script processes a dataset to detect scene boundaries and analyze character interactions in a narrative. It identifies character pairs who appear together in the same scene and calculates their connection frequency. The results are saved to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                pair  count\n",
      "0    (katara, sokka)    386\n",
      "1       (iroh, zuko)    112\n",
      "2     (aang, katara)    411\n",
      "3      (aang, sokka)    409\n",
      "4      (aang, kanna)      3\n",
      "..               ...    ...\n",
      "963    (iroh, pakku)      1\n",
      "964     (bumi, iroh)      1\n",
      "965     (ozai, suki)      1\n",
      "966      (mai, toph)      1\n",
      "967      (iroh, mai)      1\n",
      "\n",
      "[968 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each row to detect scene boundaries\n",
    "for _, row in df.iterrows():\n",
    "    # Check if the row indicates a new scene\n",
    "    if row['character'] == 'Scene Description':\n",
    "        # Create character pairs for the completed scene\n",
    "        pairs = combinations(scene_characters, 2)\n",
    "        for pair in pairs:\n",
    "            pair = tuple(sorted(pair))  # Ensure consistent ordering\n",
    "            if pair in character_connections:\n",
    "                character_connections[pair] += 1\n",
    "            else:\n",
    "                character_connections[pair] = 1\n",
    "\n",
    "        # Reset the scene_characters for the next scene\n",
    "        scene_characters = set()\n",
    "    else:\n",
    "        # Check for multiple characters and add each to the current scene\n",
    "        characters = split_characters(row['character'])\n",
    "        scene_characters.update(characters)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "connections_df = pd.DataFrame(list(character_connections.items()), columns=['pair', 'count'])\n",
    "\n",
    "# Display the connections\n",
    "print(connections_df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "connections_df.to_csv('character_connections.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script combines natural language processing and network analysis to explore character interactions and sentiments across scenes in multiple books. It uses a BERT-based sentiment analysis model to evaluate dialogue and calculate average sentiment within scenes. Character connections are quantified based on co-occurrence and sentiment, and the results are stored in a book-specific dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       book                   pair  count  \\\n",
      "0     Earth         (aang, katara)    113   \n",
      "1     Earth          (aang, pakku)      1   \n",
      "2     Earth        (katara, pakku)      1   \n",
      "3     Earth           (iroh, zuko)     55   \n",
      "4     Earth       (azula, captain)      2   \n",
      "...     ...                    ...    ...   \n",
      "1071  Water            (yue, zuko)      1   \n",
      "1072  Water  (aang, baboon spirit)      1   \n",
      "1073  Water            (aang, koh)      3   \n",
      "1074  Water            (yue, zhao)      1   \n",
      "1075  Water            (iroh, yue)      1   \n",
      "\n",
      "      average_sentiment_between_characters  \n",
      "0                                -0.096103  \n",
      "1                                 0.904363  \n",
      "2                                 0.904363  \n",
      "3                                -0.009508  \n",
      "4                                -0.074584  \n",
      "...                                    ...  \n",
      "1071                              0.213319  \n",
      "1072                              0.744977  \n",
      "1073                              0.259550  \n",
      "1074                             -0.159776  \n",
      "1075                              0.631177  \n",
      "\n",
      "[1076 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "# Load the BERT sentiment analysis model\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", framework=\"pt\")\n",
    "\n",
    "# Function to calculate sentiment using BERT\n",
    "def calculate_bert_sentiment(text, positive_bias=1.5, negative_bias=1.5):\n",
    "    \"\"\"\n",
    "    Calculate sentiment using a BERT model and apply bias.\n",
    "    Positive sentiments are amplified by `positive_bias`.\n",
    "    Negative sentiments are amplified by `negative_bias`.\n",
    "    \"\"\"\n",
    "    if pd.notna(text) and text.strip():\n",
    "        result = sentiment_analyzer(text[:512])[0]  # Analyze up to 512 characters\n",
    "        label = result[\"label\"]\n",
    "        score = float(result[\"score\"])\n",
    "\n",
    "        if \"POSITIVE\" in label.upper():\n",
    "            return score * positive_bias\n",
    "        elif \"NEGATIVE\" in label.upper():\n",
    "            return -score * negative_bias\n",
    "        return 0  # Neutral sentiment\n",
    "    return 0  # No valid text\n",
    "\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    if pd.notna(text) and text.strip():\n",
    "        result = sentiment_analyzer(text[:512])  # Truncate to 512 tokens for BERT\n",
    "        label = result[0]['label']\n",
    "        score = result[0]['score']\n",
    "        \n",
    "        # Map the labels to sentiments\n",
    "        if label == \"LABEL_1\":  # Positive\n",
    "            return \"positive\"\n",
    "        elif label == \"LABEL_0\":  # Negative\n",
    "            return \"negative\"\n",
    "        else:  # Add a fallback for unexpected labels\n",
    "            return \"neutral\"\n",
    "    return \"neutral\"\n",
    "\n",
    "\n",
    "# Iterate over each book\n",
    "character_connections_by_book = {}\n",
    "for book, book_group in df.groupby('book'):\n",
    "    scene_characters = set()  # Characters in the current scene\n",
    "    scene_sentiment_sum = 0  # Total sentiment for the current scene\n",
    "    total_sentences = 0  # Count of sentences for normalization\n",
    "    character_connections = {}  # Connections for this book\n",
    "\n",
    "    # Iterate over each row within the book\n",
    "    for _, row in book_group.iterrows():\n",
    "        if row['character'] == 'Scene Description':\n",
    "            if total_sentences > 0:\n",
    "                # Calculate average sentiment for the completed scene\n",
    "                average_scene_sentiment = scene_sentiment_sum / total_sentences\n",
    "            else:\n",
    "                # No character dialogue, sentiment is neutral\n",
    "                average_scene_sentiment = 0\n",
    "\n",
    "            # Create character pairs for the completed scene\n",
    "            pairs = combinations(scene_characters, 2)\n",
    "            for pair in pairs:\n",
    "                pair = tuple(sorted(pair))  \n",
    "                if pair in character_connections:\n",
    "                    # Update weight and average sentiment\n",
    "                    character_connections[pair]['count'] += 1\n",
    "                    character_connections[pair]['sentiment'] += average_scene_sentiment\n",
    "                else:\n",
    "                    character_connections[pair] = {\n",
    "                        'count': 1,\n",
    "                        'sentiment': average_scene_sentiment\n",
    "                    }\n",
    "\n",
    "            # Reset the scene_characters and sentiment trackers for the next scene\n",
    "            scene_characters = set()\n",
    "            scene_sentiment_sum = 0\n",
    "            total_sentences = 0\n",
    "        else:\n",
    "            # Add characters to the current scene\n",
    "            characters = split_characters(row['character'])\n",
    "            scene_characters.update(characters)\n",
    "\n",
    "            # Calculate BERT sentiment for this character's full text if available\n",
    "            character_text = row['character_words']\n",
    "            if pd.notna(character_text) and character_text.strip():\n",
    "                sentiment = calculate_bert_sentiment(character_text, positive_bias=1.5, negative_bias=1.5)\n",
    "                scene_sentiment_sum += sentiment\n",
    "                total_sentences += 1\n",
    "\n",
    "    # Normalize sentiment scores (average over the number of scenes)\n",
    "    for pair in character_connections:\n",
    "        connection = character_connections[pair]\n",
    "        connection['sentiment'] /= connection['count']\n",
    "\n",
    "    # Store connections for this book\n",
    "    character_connections_by_book[book] = character_connections\n",
    "\n",
    "# Convert connections_by_book to a DataFrame\n",
    "book_connections = []\n",
    "\n",
    "for book, connections in character_connections_by_book.items():\n",
    "    for pair, data in connections.items():\n",
    "        book_connections.append({\n",
    "            'book': book,\n",
    "            'pair': pair,\n",
    "            'count': data['count'],\n",
    "            'average_sentiment_between_characters': data['sentiment']\n",
    "        })\n",
    "\n",
    "book_connections_df = pd.DataFrame(book_connections)\n",
    "\n",
    "# Display the book-based connections with sentiment\n",
    "print(book_connections_df)\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "book_connections_df.to_csv('character_connections_by_book.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script performs character-level analysis by integrating sentiment evaluation and tracking their presence across scenes, episodes, and story arcs. Using a RoBERTa-based sentiment analysis model, it categorizes dialogue sentiment as positive, negative, or neutral. It calculates various metrics such as dialogue counts, scene counts, episode appearances, arc presence, and sentiment proportions for each character. The processed data is saved into a CSV file for detailed exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            character  dialogue_count  scene_count  episode_count  \\\n",
      "0              katara           15067          875             59   \n",
      "1               sokka           18392         1009             59   \n",
      "2                zuko            9277          466             47   \n",
      "3                iroh            5276          198             36   \n",
      "4                aang           17947         1120             60   \n",
      "..                ...             ...          ...            ...   \n",
      "345          yangchen              90            1              1   \n",
      "346       lion turtle              78            4              2   \n",
      "347  banished servant              33            1              1   \n",
      "348    head of dai li              28            1              1   \n",
      "349           qin lee              10            1              1   \n",
      "\n",
      "           arc_presence  positive_proportion  negative_proportion  \\\n",
      "0    Earth, Fire, Water             0.488817             0.319307   \n",
      "1    Earth, Fire, Water             0.506796             0.311657   \n",
      "2    Earth, Fire, Water             0.509001             0.355826   \n",
      "3    Earth, Fire, Water             0.399924             0.255497   \n",
      "4    Earth, Fire, Water             0.485318             0.285619   \n",
      "..                  ...                  ...                  ...   \n",
      "345                Fire             0.944444             0.000000   \n",
      "346                Fire             0.782051             0.217949   \n",
      "347                Fire             0.696970             0.303030   \n",
      "348                Fire             0.678571             0.000000   \n",
      "349                Fire             1.000000             0.000000   \n",
      "\n",
      "     neutral_proportion  \n",
      "0              0.191876  \n",
      "1              0.181546  \n",
      "2              0.135173  \n",
      "3              0.344579  \n",
      "4              0.229063  \n",
      "..                  ...  \n",
      "345            0.055556  \n",
      "346            0.000000  \n",
      "347            0.000000  \n",
      "348            0.321429  \n",
      "349            0.000000  \n",
      "\n",
      "[350 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", framework=\"pt\")\n",
    "\n",
    "\n",
    "# Prepare the attributes dictionary\n",
    "character_attributes = {}\n",
    "\n",
    "# Keep track of the current arc (book) and scene characters\n",
    "current_book = None\n",
    "scene_characters = set()\n",
    "\n",
    "# Sentiment Analysis Function using BERT\n",
    "def calculate_sentiment(text):\n",
    "    if pd.notna(text) and text.strip():\n",
    "        result = sentiment_analyzer(text[:512])  # Truncate to 512 tokens for BERT\n",
    "        label = result[0]['label']\n",
    "        score = result[0]['score']\n",
    "        \n",
    "        # Map the labels to sentiments\n",
    "        if label == \"LABEL_1\":  # Positive\n",
    "            return \"positive\"\n",
    "        elif label == \"LABEL_0\":  # Negative\n",
    "            return \"negative\"\n",
    "        else:  # Add a fallback for unexpected labels\n",
    "            return \"neutral\"\n",
    "    return \"neutral\"\n",
    "\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notna(row['book']):\n",
    "        current_book = row['book']\n",
    "\n",
    "    if row['character'] == 'Scene Description':\n",
    "        # At the end of the scene, increment scene counts for all characters in that scene\n",
    "        for character in scene_characters:\n",
    "            if character in character_attributes:\n",
    "                character_attributes[character]['scene_count'] += 1\n",
    "        scene_characters = set()  # Reset for the next scene\n",
    "    else:\n",
    "        # Track characters in the current scene\n",
    "        characters = split_characters(row['character'])\n",
    "        scene_characters.update(characters)\n",
    "\n",
    "        for character in characters:\n",
    "            if character not in character_attributes:\n",
    "                character_attributes[character] = {\n",
    "                    'dialogue_count': 0,\n",
    "                    'scene_count': 0,\n",
    "                    'episode_count': set(),\n",
    "                    'arc_presence': set(),\n",
    "                    'positive_dialogue': 0,\n",
    "                    'negative_dialogue': 0,\n",
    "                    'neutral_dialogue': 0\n",
    "                }\n",
    "\n",
    "            # Increment dialogue count by the number of words in the dialogue\n",
    "            dialogue = row['character_words']\n",
    "            word_count = len(dialogue.split()) if pd.notna(dialogue) else 0\n",
    "            character_attributes[character]['dialogue_count'] += word_count\n",
    "\n",
    "            # Track episodes\n",
    "            character_attributes[character]['episode_count'].add((row['book'], row['chapter']))\n",
    "\n",
    "            # Track arc presence if there's a known current_book\n",
    "            if current_book is not None:\n",
    "                character_attributes[character]['arc_presence'].add(current_book)\n",
    "\n",
    "            # Update sentiment analysis\n",
    "            sentiment_category = calculate_sentiment(dialogue)\n",
    "            if sentiment_category == \"positive\":\n",
    "                character_attributes[character]['positive_dialogue'] += word_count\n",
    "            elif sentiment_category == \"negative\":\n",
    "                character_attributes[character]['negative_dialogue'] += word_count\n",
    "            else:\n",
    "                character_attributes[character]['neutral_dialogue'] += word_count\n",
    "\n",
    "# Manually update Momo and Appa to appear in all arcs\n",
    "# First, find all arcs (books) present in the DataFrame\n",
    "all_arcs = set(df['book'].dropna())\n",
    "\n",
    "for character_name in ['momo', 'appa']:\n",
    "    if character_name in character_attributes:\n",
    "        character_attributes[character_name]['arc_presence'] = all_arcs\n",
    "\n",
    "# Process the attributes into a DataFrame\n",
    "processed_attributes = []\n",
    "for character, attributes in character_attributes.items():\n",
    "    total_dialogue = attributes['dialogue_count']\n",
    "    processed_attributes.append({\n",
    "        'character': character,\n",
    "        'dialogue_count': total_dialogue,\n",
    "        'scene_count': attributes['scene_count'],\n",
    "        'episode_count': len(attributes['episode_count']),\n",
    "        'arc_presence': ', '.join(sorted(attributes['arc_presence'])),\n",
    "        'positive_proportion': (attributes['positive_dialogue'] / total_dialogue) if total_dialogue > 0 else 0,\n",
    "        'negative_proportion': (attributes['negative_dialogue'] / total_dialogue) if total_dialogue > 0 else 0,\n",
    "        'neutral_proportion': (attributes['neutral_dialogue'] / total_dialogue) if total_dialogue > 0 else 0\n",
    "    })\n",
    "\n",
    "character_df = pd.DataFrame(processed_attributes)\n",
    "\n",
    "# Save to CSV\n",
    "character_df.to_csv('character_attributes.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(character_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       arc       character  dialogue_count  scene_count  episode_count  \\\n",
      "0    Water          katara            6275          368             20   \n",
      "1    Water           sokka            5980          374             20   \n",
      "2    Water            zuko            1773          122             13   \n",
      "3    Water            iroh            1814           85             13   \n",
      "4    Water            aang            7810          508             20   \n",
      "..     ...             ...             ...          ...            ...   \n",
      "428   Fire  head of dai li              28            1              1   \n",
      "429   Fire         qin lee              10            1              1   \n",
      "430   Fire        engineer              30            2              1   \n",
      "431   Fire     crew member              12            1              1   \n",
      "432   Fire            ursa              45            1              1   \n",
      "\n",
      "     positive_proportion  negative_proportion  neutral_proportion  \n",
      "0               0.487171             0.291793            0.221036  \n",
      "1               0.537960             0.327926            0.134114  \n",
      "2               0.567964             0.274676            0.157360  \n",
      "3               0.366042             0.353363            0.280595  \n",
      "4               0.493086             0.263124            0.243790  \n",
      "..                   ...                  ...                 ...  \n",
      "428             0.678571             0.000000            0.321429  \n",
      "429             1.000000             0.000000            0.000000  \n",
      "430             0.933333             0.000000            0.066667  \n",
      "431             0.000000             0.000000            1.000000  \n",
      "432             0.200000             0.644444            0.155556  \n",
      "\n",
      "[433 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", framework=\"pt\")\n",
    "\n",
    "\n",
    "# Prepare the attributes dictionary\n",
    "character_attributes_by_arc = {}\n",
    "\n",
    "# Keep track of the current arc (book) and scene characters\n",
    "current_book = None\n",
    "scene_characters = set()\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    if pd.notna(text) and text.strip():\n",
    "        result = sentiment_analyzer(text[:512])  # Truncate to 512 tokens for BERT\n",
    "        label = result[0]['label']\n",
    "        score = result[0]['score']\n",
    "        \n",
    "        # Map the labels to sentiments\n",
    "        if label == \"LABEL_1\":  # Positive\n",
    "            return \"positive\"\n",
    "        elif label == \"LABEL_0\":  # Negative\n",
    "            return \"negative\"\n",
    "        else:  # Add a fallback for unexpected labels\n",
    "            return \"neutral\"\n",
    "    return \"neutral\"\n",
    "\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notna(row['book']):\n",
    "        current_book = row['book']  # Update the current book (arc)\n",
    "\n",
    "    if row['character'] == 'Scene Description':\n",
    "        # At the end of the scene, increment scene counts for all characters in that scene\n",
    "        for character in scene_characters:\n",
    "            if character in character_attributes_by_arc[current_book]:\n",
    "                character_attributes_by_arc[current_book][character]['scene_count'] += 1\n",
    "        scene_characters = set()  # Reset for the next scene\n",
    "    else:\n",
    "        # Track characters in the current scene\n",
    "        characters = split_characters(row['character'])\n",
    "        scene_characters.update(characters)\n",
    "\n",
    "        for character in characters:\n",
    "            # Initialize character data for this arc if not already done\n",
    "            if current_book not in character_attributes_by_arc:\n",
    "                character_attributes_by_arc[current_book] = {}\n",
    "            if character not in character_attributes_by_arc[current_book]:\n",
    "                character_attributes_by_arc[current_book][character] = {\n",
    "                    'dialogue_count': 0,\n",
    "                    'scene_count': 0,\n",
    "                    'episode_count': set(),\n",
    "                    'positive_dialogue': 0,\n",
    "                    'negative_dialogue': 0,\n",
    "                    'neutral_dialogue': 0\n",
    "                }\n",
    "\n",
    "            # Increment dialogue count by the number of words in the dialogue\n",
    "            dialogue = row['character_words']\n",
    "            word_count = len(dialogue.split()) if pd.notna(dialogue) else 0\n",
    "            character_attributes_by_arc[current_book][character]['dialogue_count'] += word_count\n",
    "\n",
    "            # Track episodes\n",
    "            character_attributes_by_arc[current_book][character]['episode_count'].add(row['chapter'])\n",
    "\n",
    "            # Update sentiment analysis\n",
    "            sentiment_category = calculate_sentiment(dialogue)\n",
    "            if sentiment_category == \"positive\":\n",
    "                character_attributes_by_arc[current_book][character]['positive_dialogue'] += word_count\n",
    "            elif sentiment_category == \"negative\":\n",
    "                character_attributes_by_arc[current_book][character]['negative_dialogue'] += word_count\n",
    "            else:\n",
    "                character_attributes_by_arc[current_book][character]['neutral_dialogue'] += word_count\n",
    "\n",
    "# Manually update Momo and Appa to appear in all arcs\n",
    "# First, find all arcs (books) present in the DataFrame\n",
    "all_arcs = set(df['book'].dropna())\n",
    "\n",
    "for arc in all_arcs:\n",
    "    if arc not in character_attributes_by_arc:\n",
    "        character_attributes_by_arc[arc] = {}\n",
    "    for character_name in ['momo', 'appa']:\n",
    "        if character_name not in character_attributes_by_arc[arc]:\n",
    "            character_attributes_by_arc[arc][character_name] = {\n",
    "                'dialogue_count': 0,\n",
    "                'scene_count': 0,\n",
    "                'episode_count': set(),\n",
    "                'positive_dialogue': 0,\n",
    "                'negative_dialogue': 0,\n",
    "                'neutral_dialogue': 0\n",
    "            }\n",
    "\n",
    "# Process the attributes into a DataFrame per arc\n",
    "processed_data = []\n",
    "\n",
    "for arc, characters in character_attributes_by_arc.items():\n",
    "    for character, attributes in characters.items():\n",
    "        total_dialogue = attributes['dialogue_count']\n",
    "        processed_data.append({\n",
    "            'arc': arc,\n",
    "            'character': character,\n",
    "            'dialogue_count': total_dialogue,\n",
    "            'scene_count': attributes['scene_count'],\n",
    "            'episode_count': len(attributes['episode_count']),\n",
    "            'positive_proportion': (attributes['positive_dialogue'] / total_dialogue) if total_dialogue > 0 else 0,\n",
    "            'negative_proportion': (attributes['negative_dialogue'] / total_dialogue) if total_dialogue > 0 else 0,\n",
    "            'neutral_proportion': (attributes['neutral_dialogue'] / total_dialogue) if total_dialogue > 0 else 0\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the processed data\n",
    "character_arc_df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Save to CSV\n",
    "character_arc_df.to_csv('character_attributes_by_arc.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(character_arc_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This script uses the NRC Emotion Lexicon to analyze emotions and sentiments in text. It processes character dialogues across episodes, calculates emotion counts and proportions, and aggregates results into a dataset. The output is saved as a CSV file for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     book                                    chapter  positive  negative  \\\n",
      "0   Earth                           Appa's Lost Days  0.203125  0.152344   \n",
      "1   Earth                                 Avatar Day  0.255446  0.112871   \n",
      "2   Earth                                Bitter Work  0.245750  0.091190   \n",
      "3   Earth                  City of Walls and Secrets  0.248521  0.118343   \n",
      "4   Earth                                Lake Laogai  0.258907  0.109264   \n",
      "..    ...                                        ...       ...       ...   \n",
      "56  Water                     The Warriors of Kyoshi  0.247807  0.138158   \n",
      "57  Water                    The Waterbending Master  0.279294  0.086677   \n",
      "58  Water                    The Waterbending Scroll  0.253579  0.124744   \n",
      "59  Water  Winter Solstice, Part 1: The Spirit World  0.286585  0.115854   \n",
      "60  Water       Winter Solstice, Part 2: Avatar Roku  0.275154  0.141684   \n",
      "\n",
      "       anger      fear  anticipation     trust  surprise   sadness       joy  \\\n",
      "0   0.058594  0.105469      0.117188  0.128906  0.058594  0.070312  0.078125   \n",
      "1   0.075248  0.077228      0.112871  0.140594  0.061386  0.053465  0.077228   \n",
      "2   0.066461  0.094281      0.092736  0.148377  0.075734  0.046368  0.100464   \n",
      "3   0.048817  0.116864      0.088757  0.167160  0.035503  0.071006  0.072485   \n",
      "4   0.047506  0.054632      0.111639  0.171021  0.066508  0.061758  0.085511   \n",
      "..       ...       ...           ...       ...       ...       ...       ...   \n",
      "56  0.070175  0.059211      0.103070  0.149123  0.070175  0.050439  0.083333   \n",
      "57  0.048154  0.067416      0.107544  0.178170  0.054575  0.043339  0.110754   \n",
      "58  0.049080  0.061350      0.104294  0.165644  0.057260  0.057260  0.092025   \n",
      "59  0.054878  0.069106      0.093496  0.142276  0.048780  0.071138  0.087398   \n",
      "60  0.059548  0.143737      0.098563  0.129363  0.028747  0.049281  0.041068   \n",
      "\n",
      "     disgust  \n",
      "0   0.027344  \n",
      "1   0.033663  \n",
      "2   0.038640  \n",
      "3   0.032544  \n",
      "4   0.033254  \n",
      "..       ...  \n",
      "56  0.028509  \n",
      "57  0.024077  \n",
      "58  0.034765  \n",
      "59  0.030488  \n",
      "60  0.032854  \n",
      "\n",
      "[61 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Load the NRC Emotion Lexicon\n",
    "def load_nrc_lexicon(filepath):\n",
    "    lexicon = defaultdict(lambda: {\"positive\": 0, \"negative\": 0, \"anger\": 0, \"fear\": 0, \n",
    "                                    \"anticipation\": 0, \"trust\": 0, \"surprise\": 0, \n",
    "                                    \"sadness\": 0, \"joy\": 0, \"disgust\": 0})\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            term, category, association = line.strip().split('\\t')\n",
    "            if int(association) == 1:  # Only include words with a positive association\n",
    "                lexicon[term][category] = 1\n",
    "    return lexicon\n",
    "\n",
    "# Tokenize text into words\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Analyze sentiment/emotion for a block of text\n",
    "def analyze_text_sentiment(text, lexicon):\n",
    "    emotion_counts = defaultdict(int)\n",
    "\n",
    "    # Tokenize and count words\n",
    "    words = tokenize(text)\n",
    "    for word in words:\n",
    "        for emotion, value in lexicon[word].items():\n",
    "            emotion_counts[emotion] += value\n",
    "\n",
    "    # Calculate proportions\n",
    "    total_emotions = sum(emotion_counts.values())\n",
    "    if total_emotions > 0:\n",
    "        proportions = {k: v / total_emotions for k, v in emotion_counts.items()}\n",
    "    else:\n",
    "        proportions = {k: 0 for k in emotion_counts.keys()}\n",
    "\n",
    "    return emotion_counts, proportions\n",
    "\n",
    "# Process each episode (chapter)\n",
    "def process_episode(data, book, chapter, lexicon):\n",
    "    # Filter rows for the given book and chapter\n",
    "    episode_data = data[(data['book'] == book) & (data['chapter'] == chapter)]\n",
    "\n",
    "    # Concatenate all character dialogues\n",
    "    episode_text = ' '.join(episode_data['character_words'].dropna())\n",
    "    \n",
    "    # Analyze sentiment/emotions\n",
    "    counts, proportions = analyze_text_sentiment(episode_text, lexicon)\n",
    "    return counts, proportions\n",
    "\n",
    "\n",
    "# Filter out rows where the character is \"Scene Description\"\n",
    "df = df[df['character'] != \"Scene Description\"]\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[['book', 'chapter', 'chapter_num', 'character_words']]\n",
    "\n",
    "# Load the NRC Emotion Lexicon (update the file path)\n",
    "nrc_lexicon_path = 'NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_nrc_lexicon(nrc_lexicon_path)\n",
    "\n",
    "# Analyze all episodes\n",
    "results = []\n",
    "\n",
    "for (book, chapter), group in df.groupby(['book', 'chapter']):\n",
    "    counts, proportions = process_episode(df, book, chapter, nrc_lexicon)\n",
    "    results.append({\n",
    "        'book': book,\n",
    "        'chapter': chapter,\n",
    "        'positive': proportions.get('positive', 0),\n",
    "        'negative': proportions.get('negative', 0),\n",
    "        'anger': proportions.get('anger', 0),\n",
    "        'fear': proportions.get('fear', 0),\n",
    "        'anticipation': proportions.get('anticipation', 0),\n",
    "        'trust': proportions.get('trust', 0),\n",
    "        'surprise': proportions.get('surprise', 0),\n",
    "        'sadness': proportions.get('sadness', 0),\n",
    "        'joy': proportions.get('joy', 0),\n",
    "        'disgust': proportions.get('disgust', 0)\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "emotion_results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to a CSV file\n",
    "emotion_results_df.to_csv('episode_emotions.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(emotion_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes IMDb ratings for episodes, aggregates ratings by unique episodes, and identifies the top 10 best and worst-rated episodes. The results are sorted and saved as separate CSV files for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Best-Rated Episodes:\n",
      "     book                                    chapter  chapter_num  imdb_rating\n",
      "24   Fire    Sozin's Comet, Part 3: Into the Inferno           20          9.8\n",
      "25   Fire         Sozin's Comet, Part 4: Avatar Aang           21          9.8\n",
      "10  Earth                  The Crossroads of Destiny           20          9.6\n",
      "19  Earth                                 Zuko Alone            7          9.5\n",
      "26   Fire               The Avatar and the Fire Lord            6          9.5\n",
      "23   Fire     Sozin's Comet, Part 2: The Old Masters           19          9.5\n",
      "52  Water             The Siege of the North, Part 1           19          9.4\n",
      "32   Fire  The Day of Black Sun, Part 2: The Eclipse           11          9.4\n",
      "30   Fire                   The Boiling Rock, Part 2           15          9.2\n",
      "18  Earth                    The Tales of Ba Sing Se           15          9.2\n",
      "\n",
      "Top 10 Worst-Rated Episodes:\n",
      "     book                   chapter  chapter_num  imdb_rating\n",
      "57  Water   The Waterbending Scroll            9          8.0\n",
      "36   Fire          The Painted Lady            3          8.0\n",
      "42  Water                Imprisoned            6          8.0\n",
      "48  Water         The Fortuneteller           14          7.9\n",
      "41  Water   Bato of the Water Tribe           15          7.9\n",
      "20   Fire  Nightmares and Daydreams            9          7.8\n",
      "43  Water                       Jet           10          7.8\n",
      "17  Earth                 The Swamp            4          7.7\n",
      "1   Earth                Avatar Day            5          7.5\n",
      "49  Water          The Great Divide           11          7.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "imdb_df = pd.read_csv('data/avatar.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Ensure the `imdb_rating` column is treated as numeric\n",
    "imdb_df['imdb_rating'] = pd.to_numeric(imdb_df['imdb_rating'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing or invalid ratings\n",
    "imdb_df = imdb_df.dropna(subset=['imdb_rating'])\n",
    "\n",
    "# Aggregate by unique episodes using `book`, `chapter`, and `chapter_num`\n",
    "# Take the mean `imdb_rating` for episodes with multiple rows\n",
    "aggregated_df = imdb_df.groupby(['book', 'chapter', 'chapter_num'], as_index=False).agg({\n",
    "    'imdb_rating': 'mean'\n",
    "})\n",
    "\n",
    "# Sort by rating\n",
    "sorted_imdb = aggregated_df.sort_values(by='imdb_rating', ascending=False)\n",
    "\n",
    "# Top 10 best-rated episodes\n",
    "top_10_best = sorted_imdb.head(10)\n",
    "\n",
    "# Top 10 worst-rated episodes\n",
    "top_10_worst = sorted_imdb.tail(10)\n",
    "\n",
    "\n",
    "# Save results to separate CSV files\n",
    "top_10_best.to_csv('top_10_best_rated_episodes.csv', index=False)\n",
    "top_10_worst.to_csv('top_10_worst_rated_episodes.csv', index=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Best-Rated Episodes:\")\n",
    "print(top_10_best)\n",
    "\n",
    "print(\"\\nTop 10 Worst-Rated Episodes:\")\n",
    "print(top_10_worst)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
